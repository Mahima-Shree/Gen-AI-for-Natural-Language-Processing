{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahima-Shree/Gen-AI-for-Natural-Language-Processing/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "E_TEXT = \"Hello-Hello , - i am Mahima Shree\"\n",
        "\n",
        "a= word_tokenize(E_TEXT)\n",
        "# type(a)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7QstBl5woVL",
        "outputId": "fa9f9f78-d66a-477c-a4b7-59cbe45d6a5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello-Hello', ',', '-', 'i', 'am', 'Mahima', 'Shree']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "S2_TEXT = \"Positive thinking is all. A matter of habits If you are; not quite a positive thinker Change Yourself?\"\n",
        "\n",
        "print(sent_tokenize(S2_TEXT))\n",
        "\n",
        "## type(sent_tokenize(E_TEXT)) ##that !endspace ,?,."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc8IcUXnxIEj",
        "outputId": "44be2a22-7143-48d1-fe03-4fbf77de5eb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Positive thinking is all.', 'A matter of habits If you are; not quite a positive thinker Change Yourself?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  store the words and sentences and type cast it:\n",
        "\n",
        "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
        "\n",
        "phrases = sent_tokenize(data)\n",
        "words = word_tokenize(data)\n",
        "\n",
        "\n",
        "new_array=np.array(words)\n",
        "new_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRIxvTQQzhZQ",
        "outputId": "cd4369e3-83a3-46e7-9dd9-e541d73184a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy',\n",
              "       '.', 'All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a',\n",
              "       'dull', 'boy', '.'], dtype='<U5')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "a = \"I think i that Learning  DATA Science will bring a big leap in your Carrier Profile. Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains\"\n",
        "word_tokens = word_tokenize(a)\n",
        "word_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUI3ZbKXzhoH",
        "outputId": "5311c72e-2775-47cb-cffd-84ced7c0c787"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'think',\n",
              " 'i',\n",
              " 'that',\n",
              " 'Learning',\n",
              " 'DATA',\n",
              " 'Science',\n",
              " 'will',\n",
              " 'bring',\n",
              " 'a',\n",
              " 'big',\n",
              " 'leap',\n",
              " 'in',\n",
              " 'your',\n",
              " 'Carrier',\n",
              " 'Profile',\n",
              " '.',\n",
              " 'Data',\n",
              " 'science',\n",
              " 'is',\n",
              " 'an',\n",
              " 'interdisciplinary',\n",
              " 'field',\n",
              " 'that',\n",
              " 'uses',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " ',',\n",
              " 'processes',\n",
              " ',',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'systems',\n",
              " 'to',\n",
              " 'extract',\n",
              " 'knowledge',\n",
              " 'and',\n",
              " 'insights',\n",
              " 'from',\n",
              " 'noisy',\n",
              " ',',\n",
              " 'structured',\n",
              " 'and',\n",
              " 'unstructured',\n",
              " 'data',\n",
              " ',',\n",
              " 'and',\n",
              " 'apply',\n",
              " 'knowledge',\n",
              " 'from',\n",
              " 'data',\n",
              " 'across',\n",
              " 'a',\n",
              " 'broad',\n",
              " 'range',\n",
              " 'of',\n",
              " 'application',\n",
              " 'domains']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words1 = set(stopwords.words('english')) #downloads the file with english stop words\n",
        "word_tokens = word_tokenize(a)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words1]\n",
        "print(filtered_sentence)\n",
        "#print(word_tokens)\n",
        "#print(filtered_sentence)\n",
        "print(\"The number of words stopped :\",(len(word_tokens)-len(filtered_sentence)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVNV717f0JHx",
        "outputId": "7d7df0c1-6a70-4400-eff9-6e09360fa6ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'think', 'Learning', 'DATA', 'Science', 'bring', 'big', 'leap', 'Carrier', 'Profile', '.', 'Data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'noisy', ',', 'structured', 'unstructured', 'data', ',', 'apply', 'knowledge', 'data', 'across', 'broad', 'range', 'application', 'domains']\n",
            "The number of words stopped : 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=[\"I\",\".\",\",\"]  #Creating your own Stop word list\n",
        "stop_words1=list(stop_words1)\n",
        "stop_words2 = b #downloads the file with english stop words\n",
        "stop_words=stop_words1+stop_words2\n",
        "word_tokens = word_tokenize(a)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "print(filtered_sentence)\n",
        "\n",
        "#print(word_tokens)\n",
        "#print(filtered_sentence)\n",
        "print(\"The number of words stopped :\",(len(word_tokens)-len(filtered_sentence)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZdE6SLJ0JT_",
        "outputId": "afc2acde-d7d7-4500-cbbc-c1222cde7ad0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['think', 'Learning', 'DATA', 'Science', 'bring', 'big', 'leap', 'Carrier', 'Profile', 'Data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', 'processes', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'noisy', 'structured', 'unstructured', 'data', 'apply', 'knowledge', 'data', 'across', 'broad', 'range', 'application', 'domains']\n",
            "The number of words stopped : 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LAB Questions**"
      ],
      "metadata": {
        "id": "Z_LDCvNylNxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = '20 years ago, we all found in different ways in different places but all at the same moment.'\n",
        "data2 = 'That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.'\n",
        "data3 = 'These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.'\n",
        "data4 = 'There was horror at the scale of destruction. and awe at the bravery and kindness that rose to meet it. There was, shock! at the audacity of evil and gratitude, for the heroism and decency that opposed it? In the sacrifice!'"
      ],
      "metadata": {
        "id": "8na_p8vQ0Zgm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "XydkwtZl2ehd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens1 = word_tokenize(data1)\n",
        "word_tokens2 = word_tokenize(data2)\n",
        "word_tokens3 = word_tokenize(data3)\n",
        "word_tokens4 = word_tokenize(data4)\n"
      ],
      "metadata": {
        "id": "muNwWseA3C7O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokens1)\n",
        "print(word_tokens2)\n",
        "print(word_tokens3)\n",
        "print(word_tokens4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nY3y4aw3C-s",
        "outputId": "b158300e-ced2-4625-dbeb-f8ff84868544"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['20', 'years', 'ago', ',', 'we', 'all', 'found', 'in', 'different', 'ways', 'in', 'different', 'places', 'but', 'all', 'at', 'the', 'same', 'moment', '.']\n",
            "['That', 'our', 'lives', 'would', 'be', 'changed', 'forever', '.', 'The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirens', 'and', 'then', 'quiet', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again', '.']\n",
            "['These', 'lives', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you', '.', 'Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well', '.', 'For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feelings', 'we', 'experienced', '.']\n",
            "['There', 'was', 'horror', 'at', 'the', 'scale', 'of', 'destruction', '.', 'and', 'awe', 'at', 'the', 'bravery', 'and', 'kindness', 'that', 'rose', 'to', 'meet', 'it', '.', 'There', 'was', ',', 'shock', '!', 'at', 'the', 'audacity', 'of', 'evil', 'and', 'gratitude', ',', 'for', 'the', 'heroism', 'and', 'decency', 'that', 'opposed', 'it', '?', 'In', 'the', 'sacrifice', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens1 = sent_tokenize(data1)\n",
        "sent_tokens2 = sent_tokenize(data2)\n",
        "sent_tokens3 = sent_tokenize(data3)\n",
        "sent_tokens4 = sent_tokenize(data4)"
      ],
      "metadata": {
        "id": "-lNy6oTM9Ahq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokens1)\n",
        "print(sent_tokens2)\n",
        "print(sent_tokens3)\n",
        "print(sent_tokens4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RIVf9e_9Axg",
        "outputId": "84a2a8c9-af6b-4ce5-db98-a36e2886dedf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['20 years ago, we all found in different ways in different places but all at the same moment.']\n",
            "['That our lives would be changed forever.', 'The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.']\n",
            "['These lives remain precious to our country and infinitely precious to many of you.', 'Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well.', 'For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.']\n",
            "['There was horror at the scale of destruction.', 'and awe at the bravery and kindness that rose to meet it.', 'There was, shock!', 'at the audacity of evil and gratitude, for the heroism and decency that opposed it?', 'In the sacrifice!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "4HZUB1sk2GcX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words1 = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentence1 = [w for w in word_tokens1 if not w in stop_words1]\n",
        "print(word_tokens1)\n",
        "print(filtered_sentence1)\n",
        "print(\"The number of words stopped :\",(len(word_tokens1)-len(filtered_sentence1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMu3lNgR9v1W",
        "outputId": "08ed5d49-d1f4-446a-bffa-f6ba610ccdd7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['20', 'years', 'ago', ',', 'we', 'all', 'found', 'in', 'different', 'ways', 'in', 'different', 'places', 'but', 'all', 'at', 'the', 'same', 'moment', '.']\n",
            "['20', 'years', 'ago', ',', 'found', 'different', 'ways', 'different', 'places', 'moment', '.']\n",
            "The number of words stopped : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words2 = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentence2 = [w for w in word_tokens2 if not w in stop_words2]\n",
        "print(word_tokens2)\n",
        "print(filtered_sentence2)\n",
        "print(\"The number of words stopped :\",(len(word_tokens2)-len(filtered_sentence2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUv5kjhz_Llu",
        "outputId": "b98bcd10-a336-419d-a2f8-5a6540c61bc3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['That', 'our', 'lives', 'would', 'be', 'changed', 'forever', '.', 'The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirens', 'and', 'then', 'quiet', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again', '.']\n",
            "['That', 'lives', 'would', 'changed', 'forever', '.', 'The', 'world', 'loud', 'carnage', 'sirens', 'quiet', 'missing', 'voices', 'would', 'never', 'heard', '.']\n",
            "The number of words stopped : 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words3 = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentence3 = [w for w in word_tokens3 if not w in stop_words3]\n",
        "print(word_tokens3)\n",
        "print(filtered_sentence3)\n",
        "print(\"The number of words stopped :\",(len(word_tokens3)-len(filtered_sentence3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k38mE0-4_LpE",
        "outputId": "c1210b66-db74-49aa-f8df-258fcd7b4286"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['These', 'lives', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you', '.', 'Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well', '.', 'For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feelings', 'we', 'experienced', '.']\n",
            "['These', 'lives', 'remain', 'precious', 'country', 'infinitely', 'precious', 'many', '.', 'Today', ',', 'remember', 'loss', ',', 'share', 'sorrow', ',', 'honor', 'men', 'women', 'loved', 'long', 'well', '.', 'For', 'young', 'recall', 'clear', 'September', 'day', ',', 'hard', 'describe', 'mix', 'feelings', 'experienced', '.']\n",
            "The number of words stopped : 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words4 = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentence4 = [w for w in word_tokens4 if not w in stop_words4]\n",
        "print(word_tokens4)\n",
        "print(filtered_sentence4)\n",
        "print(\"The number of words stopped :\",(len(word_tokens4)-len(filtered_sentence4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnzZtpE3_L6m",
        "outputId": "e1353e86-a1d9-4e10-d0d4-6a5e41a2183e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'was', 'horror', 'at', 'the', 'scale', 'of', 'destruction', '.', 'and', 'awe', 'at', 'the', 'bravery', 'and', 'kindness', 'that', 'rose', 'to', 'meet', 'it', '.', 'There', 'was', ',', 'shock', '!', 'at', 'the', 'audacity', 'of', 'evil', 'and', 'gratitude', ',', 'for', 'the', 'heroism', 'and', 'decency', 'that', 'opposed', 'it', '?', 'In', 'the', 'sacrifice', '!']\n",
            "['There', 'horror', 'scale', 'destruction', '.', 'awe', 'bravery', 'kindness', 'rose', 'meet', '.', 'There', ',', 'shock', '!', 'audacity', 'evil', 'gratitude', ',', 'heroism', 'decency', 'opposed', '?', 'In', 'sacrifice', '!']\n",
            "The number of words stopped : 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_stop_words=[\".\",\",\",\"!\"]"
      ],
      "metadata": {
        "id": "RClKP2NS_MO4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words1=list(stop_words1)\n",
        "stop_words2 = extra_stop_words\n",
        "stop_words=stop_words1+stop_words2\n",
        "\n",
        "filtered_sentence1 = [w for w in word_tokens1 if not w in stop_words]\n",
        "print(filtered_sentence1)\n",
        "\n",
        "#print(word_tokens)\n",
        "#print(filtered_sentence)\n",
        "print(\"The number of words stopped :\",(len(word_tokens1)-len(filtered_sentence1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmZmMjdC9wB9",
        "outputId": "30044036-40db-44cb-a138-488c02a8538d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['20', 'years', 'ago', 'found', 'different', 'ways', 'different', 'places', 'moment']\n",
            "The number of words stopped : 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgO67GKP-7KX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}